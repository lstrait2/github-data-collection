{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from classifier import *\n",
    "from heapq import nlargest\n",
    "from issues import get_num_code_lines\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/eclipse/eclipse_issues.json') as json_data:\n",
    "    issues = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 440743\n",
      "Number of labeled issues: 216364\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of issues: \" + str(len(issues)))\n",
    "labeled_issues = [issue for issue in issues if issue['completed_by']]\n",
    "print(\"Number of labeled issues: \" + str(len(labeled_issues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>completed_by</th>\n",
       "      <th>component</th>\n",
       "      <th>created_date</th>\n",
       "      <th>product</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214155</th>\n",
       "      <td>The new page is not offering versioning report...</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>nboldt</td>\n",
       "      <td>releng\\n\\n  (show other bugs)\\n</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>WTP Releng\\n\\n</td>\n",
       "      <td>Versioning Report for new builds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164418</th>\n",
       "      <td>I received this NPE when comparing two project...</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>loskutov</td>\n",
       "      <td>Compare\\n\\n  (show other bugs)\\n</td>\n",
       "      <td>2010-09-23</td>\n",
       "      <td>Platform\\n\\n</td>\n",
       "      <td>NPE when comparing two projects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body closed_date  \\\n",
       "214155  The new page is not offering versioning report...  2018-07-05   \n",
       "164418  I received this NPE when comparing two project...  2018-07-09   \n",
       "\n",
       "       completed_by                         component created_date  \\\n",
       "214155       nboldt   releng\\n\\n  (show other bugs)\\n   2014-08-19   \n",
       "164418     loskutov  Compare\\n\\n  (show other bugs)\\n   2010-09-23   \n",
       "\n",
       "               product                             title  \n",
       "214155  WTP Releng\\n\\n  Versioning Report for new builds  \n",
       "164418    Platform\\n\\n   NPE when comparing two projects  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame from the list of issues\n",
    "df_list = []\n",
    "for issue in labeled_issues:\n",
    "        df_dict = {}\n",
    "        df_dict['title'] = issue['short_desc'].replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        df_dict['body'] = issue['long_desc'].replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        df_dict['closed_date'] = pd.to_datetime(issue['completed_at'][:10])\n",
    "        df_dict['created_date'] = pd.to_datetime(issue['created_at'][:10])\n",
    "        df_dict['completed_by'] = issue['completed_by']\n",
    "        df_dict['product'] = issue['product']\n",
    "        df_dict['component'] = issue['component']\n",
    "        df_list.append(df_dict)\n",
    "df = pd.DataFrame(df_list).sort_values('closed_date')\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>completed_by</th>\n",
       "      <th>created_date</th>\n",
       "      <th>title</th>\n",
       "      <th>component_AGF\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AGF Chart\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AI\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AJBrowser\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AJDoc\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>...</th>\n",
       "      <th>product_Web Tools</th>\n",
       "      <th>product_WindowBuilder</th>\n",
       "      <th>product_Woolsey</th>\n",
       "      <th>product_Working Groups</th>\n",
       "      <th>product_XWT</th>\n",
       "      <th>product_Xtend</th>\n",
       "      <th>product_e4</th>\n",
       "      <th>product_eTrice</th>\n",
       "      <th>product_m2e</th>\n",
       "      <th>product_z_Archived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>- new java project- project properties / Java ...</td>\n",
       "      <td>2001-10-11</td>\n",
       "      <td>Claude_Knaus</td>\n",
       "      <td>2001-10-10</td>\n",
       "      <td>Classpath variable selection dialog has wrong ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>To reproduce:Perform a refactoring on a Java r...</td>\n",
       "      <td>2001-10-11</td>\n",
       "      <td>akiezun</td>\n",
       "      <td>2001-10-10</td>\n",
       "      <td>Missing up/down arrows in the refactoring sour...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body closed_date  \\\n",
       "1240  - new java project- project properties / Java ...  2001-10-11   \n",
       "1022  To reproduce:Perform a refactoring on a Java r...  2001-10-11   \n",
       "\n",
       "      completed_by created_date  \\\n",
       "1240  Claude_Knaus   2001-10-10   \n",
       "1022       akiezun   2001-10-10   \n",
       "\n",
       "                                                  title  \\\n",
       "1240  Classpath variable selection dialog has wrong ...   \n",
       "1022  Missing up/down arrows in the refactoring sour...   \n",
       "\n",
       "      component_AGF\\n\\n  (show other bugs)\\n  \\\n",
       "1240                                       0   \n",
       "1022                                       0   \n",
       "\n",
       "      component_AGF Chart\\n\\n  (show other bugs)\\n  \\\n",
       "1240                                             0   \n",
       "1022                                             0   \n",
       "\n",
       "      component_AI\\n\\n  (show other bugs)\\n  \\\n",
       "1240                                      0   \n",
       "1022                                      0   \n",
       "\n",
       "      component_AJBrowser\\n\\n  (show other bugs)\\n  \\\n",
       "1240                                             0   \n",
       "1022                                             0   \n",
       "\n",
       "      component_AJDoc\\n\\n  (show other bugs)\\n           ...            \\\n",
       "1240                                         0           ...             \n",
       "1022                                         0           ...             \n",
       "\n",
       "      product_Web Tools\\n\\n  product_WindowBuilder\\n\\n  product_Woolsey\\n\\n  \\\n",
       "1240                      0                          0                    0   \n",
       "1022                      0                          0                    0   \n",
       "\n",
       "      product_Working Groups\\n\\n  product_XWT\\n\\n  product_Xtend\\n\\n  \\\n",
       "1240                           0                0                  0   \n",
       "1022                           0                0                  0   \n",
       "\n",
       "      product_e4\\n\\n  product_eTrice\\n\\n  product_m2e\\n\\n  \\\n",
       "1240               0                   0                0   \n",
       "1022               0                   0                0   \n",
       "\n",
       "      product_z_Archived\\n\\n  \n",
       "1240                       0  \n",
       "1022                       0  \n",
       "\n",
       "[2 rows x 1010 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode the component and team columns\n",
    "df = pd.get_dummies(df, columns=[\"component\", \"product\"], prefix=[\"component\", \"product\"])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82093\n"
     ]
    }
   ],
   "source": [
    "size = 210000\n",
    "#get list of devs who solve an issue in last three months (from end of train set)\n",
    "active_devs = set()\n",
    "dev_counts = {}\n",
    "filter_date = df.iloc[size]['closed_date'] - pd.to_timedelta(90, unit='d')\n",
    "for _, row in df.iterrows():\n",
    "    if (row['closed_date'] > filter_date):\n",
    "        dev_counts[row['completed_by']] = dev_counts.get(row['completed_by'], 0) + 1\n",
    "#print(dev_counts)\n",
    "for dev in dev_counts:\n",
    "    if dev_counts[dev] >= 3:\n",
    "        active_devs.add(dev)\n",
    "#print(active_devs)\n",
    "# remove all issues not solved by an active dev\n",
    "df = df[df['completed_by'].isin(active_devs)].reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    # remove punctuation and non-alpha numeric characters\n",
    "    split1 = ' '.join([word for word in re.split('\\W+', text) if word.isalpha()])\n",
    "    # split camel case words apart (necessary for embedded code) and apply stemmer to all words\n",
    "    split2 = ' '.join([stemmer.stem(word) for word in re.sub('(?!^)([A-Z][a-z]+)', r' \\1', split1).split()])\n",
    "    return split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82093, 59589)\n",
      "(82093, 10429)\n"
     ]
    }
   ],
   "source": [
    "# use tf-idf w/ stemming, stop-word removal, and non-alphabetic word removal to generate features\n",
    "df['body'] = df['body'].apply(preprocess)\n",
    "vectorizer_body = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vectorizer_body.fit(df['body'])\n",
    "vector_body = vectorizer_body.transform(df['body'])\n",
    "# summarize encoded vector\n",
    "print(vector_body.shape)\n",
    "df['title'] = df['title'].apply(preprocess)\n",
    "vectorizer_title = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vectorizer_title.fit(df['title'])\n",
    "vector_title = vectorizer_title.transform(df['title'])\n",
    "# summarize encoded vector\n",
    "print(vector_title.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(82093, 70018)\n",
      "(82093, 70018)\n",
      "(82093, 71023)\n"
     ]
    }
   ],
   "source": [
    "print(type(vector_title))\n",
    "print(type(vector_body))\n",
    "sparse_data = hstack((vector_title, vector_body))\n",
    "print(sparse_data.shape)\n",
    "#TODO: these should be numerical features before combining\n",
    "df['completed_by'] = df['completed_by'].astype('category')\n",
    "df['completed_by_encode'] = df['completed_by'].cat.codes\n",
    "#df['completed_by'] = pd.factorize(df['completed_by'])\n",
    "#sparse_data = hstack((sparse_data,np.array(df['completed_by_encode'])[:,None]))\n",
    "print(sparse_data.shape)\n",
    "#TODO: add component and team\n",
    "filter_cols = [col for col in df if col.startswith('product') or col.startswith('component')]\n",
    "sparse_data = hstack((sparse_data,np.array(df[filter_cols]))).tocsr()\n",
    "print(sparse_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_df = df[:size]\\ntest_df = df[size:]\\nX_train = train_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\\ny_train = train_df['completed_by']\\nX_test = test_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\\ny_test = test_df['completed_by']\\n\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 80000\n",
    "X_train = sparse_data[:size]\n",
    "X_test = sparse_data[size:]\n",
    "y_train = df['completed_by'][:size]\n",
    "y_test = df['completed_by'][size:]\n",
    "\n",
    "'''\n",
    "train_df = df[:size]\n",
    "test_df = df[size:]\n",
    "X_train = train_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\n",
    "y_train = train_df['completed_by']\n",
    "X_test = test_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\n",
    "y_test = test_df['completed_by']\n",
    "'''\n",
    "#print(len(X_train))\n",
    "#print(len(X_test))\n",
    "#X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_top_k(pred_prob, k, actual, labels):\n",
    "    indices = [i for i in range(len(pred_prob))]\n",
    "    top_indices = nlargest(k, indices, key=lambda i: pred_prob[i])\n",
    "    top_choices = set([labels[i] for i in top_indices])\n",
    "    return actual in top_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(25,25))\n",
    "classifier.fit(X_train,y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classifier = RandomForestClassifier(n_estimators=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try SVC (might need to do something else)\n",
    "'''\n",
    "classifier = SVC(probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(25,25))\n",
    "classifier.fit(X_train,y_train)\n",
    "preds = classifier.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "pd.value_counts(actual).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['completed_by']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(preds).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_counts = pd.value_counts(df['completed_by'])\n",
    "# list of devs sorted in order of highest contribution\n",
    "sorted_dev_list = overall_counts.index.values\n",
    "pred_counts = pd.value_counts(preds)\n",
    "actual_counts = pd.value_counts(actual)\n",
    "\n",
    "percent_diff = {}\n",
    "vals = []\n",
    "counts = []\n",
    "missing = set()\n",
    "for dev in sorted_dev_list:\n",
    "    if dev in pred_counts:\n",
    "        val = 100 * (pred_counts[dev] - actual_counts[dev]) / (actual_counts[dev])\n",
    "        percent_diff[dev] = val\n",
    "        counts.append(overall_counts[dev])\n",
    "        vals.append(val)\n",
    "    else:\n",
    "        missing.add(dev)\n",
    "print(\"devs not included in predictions: \")\n",
    "print(list(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(percent_diff.keys(), percent_diff.values())\n",
    "plt.title(\"Percent Difference Prediction Rate vs. Actual Rate\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "#del vals[2]\n",
    "#del counts[2]\n",
    "# create best fit line\n",
    "print(counts)\n",
    "print(vals)\n",
    "counts2 = []\n",
    "vals2 = []\n",
    "for i, val in enumerate(vals):\n",
    "    if val == float(\"inf\") or val > 1100:\n",
    "        continue\n",
    "    counts2.append(counts[i])\n",
    "    vals2.append(vals[i])\n",
    "print(counts2)\n",
    "print(vals2)\n",
    "counts = counts2\n",
    "vals = vals2\n",
    "z = np.polyfit(x=counts, y=vals, deg=1)\n",
    "p = np.poly1d(z)\n",
    "trend_line = p(counts)\n",
    "# test best fit\n",
    "yhat = trend_line    \n",
    "ybar = np.sum(vals)/len(vals)\n",
    "ssreg = np.sum((yhat-ybar)**2) \n",
    "sstot = np.sum((vals - ybar)**2)\n",
    "print(\"R^2: \" + str(ssreg / sstot))\n",
    "# create plots\n",
    "plt.scatter(counts, vals)\n",
    "plt.title(\"Percent Difference Between Prediction Rate and Actual Rate vs. Issue Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Number of Issues Solved\")\n",
    "plt.ylabel(\"% diff. pred rate and actual rate\")\n",
    "plt.plot(counts, trend_line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: look at assigning open issues and seeing what overspecialization problem would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
