{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from classifier import *\n",
    "from heapq import nlargest\n",
    "from issues import get_num_code_lines\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: concat all these into one file\n",
    "with open('../data/eclipse/eclpise_issues1.json') as json_data:\n",
    "    issues1 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues2.json') as json_data:\n",
    "    issues2 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues3.json') as json_data:\n",
    "    issues3 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues4.json') as json_data:\n",
    "    issues4 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues5.json') as json_data:\n",
    "    issues5 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues6.json') as json_data:\n",
    "    issues6 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues7.json') as json_data:\n",
    "    issues7 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues8.json') as json_data:\n",
    "    issues8 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues9.json') as json_data:\n",
    "    issues9 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues10.json') as json_data:\n",
    "    issues10 = json.load(json_data)\n",
    "with open('../data/eclipse/eclpise_issues11.json') as json_data:\n",
    "    issues11 = json.load(json_data)\n",
    "issues = issues1 + issues2 + issues3 + issues4 + issues5 + issues6 + issues7 + issues8 + issues9 + issues10 + issues11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 173401\n",
      "Number of labeled issues: 88536\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of issues: \" + str(len(issues)))\n",
    "labeled_issues = [issue for issue in issues if issue['completed_by']]\n",
    "print(\"Number of labeled issues: \" + str(len(labeled_issues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>completed_by</th>\n",
       "      <th>component</th>\n",
       "      <th>created_date</th>\n",
       "      <th>product</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>I saw this happen on linux, but I'm sure it ha...</td>\n",
       "      <td>2001-10-11</td>\n",
       "      <td>unknown</td>\n",
       "      <td>UI\\n\\n  (show other bugs)\\n</td>\n",
       "      <td>2001-10-10</td>\n",
       "      <td>Platform\\n\\n</td>\n",
       "      <td>[UI] Unzoom occurs if you select file in navig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>DS (10/9/01 4:16:49 PM)Create two classesActio...</td>\n",
       "      <td>2001-10-11</td>\n",
       "      <td>jared_burns</td>\n",
       "      <td>Debug\\n\\n  (show other bugs)\\n</td>\n",
       "      <td>2001-10-10</td>\n",
       "      <td>JDT\\n\\n</td>\n",
       "      <td>Breakpoints installed in all classes with \"nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  body closed_date  \\\n",
       "392  I saw this happen on linux, but I'm sure it ha...  2001-10-11   \n",
       "251  DS (10/9/01 4:16:49 PM)Create two classesActio...  2001-10-11   \n",
       "\n",
       "    completed_by                       component created_date       product  \\\n",
       "392      unknown     UI\\n\\n  (show other bugs)\\n   2001-10-10  Platform\\n\\n   \n",
       "251  jared_burns  Debug\\n\\n  (show other bugs)\\n   2001-10-10       JDT\\n\\n   \n",
       "\n",
       "                                                 title  \n",
       "392  [UI] Unzoom occurs if you select file in navig...  \n",
       "251  Breakpoints installed in all classes with \"nam...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame from the list of issues\n",
    "df_list = []\n",
    "for issue in labeled_issues[25:]:\n",
    "        df_dict = {}\n",
    "        df_dict['title'] = issue['short_desc'].replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        df_dict['body'] = issue['long_desc'].replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        df_dict['closed_date'] = pd.to_datetime(issue['completed_at'][:10])\n",
    "        df_dict['created_date'] = pd.to_datetime(issue['created_at'][:10])\n",
    "        df_dict['completed_by'] = issue['completed_by']\n",
    "        df_dict['product'] = issue['product']\n",
    "        df_dict['component'] = issue['component']\n",
    "        df_list.append(df_dict)\n",
    "df = pd.DataFrame(df_list).sort_values('closed_date')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>completed_by</th>\n",
       "      <th>created_date</th>\n",
       "      <th>title</th>\n",
       "      <th>component_AJBrowser\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AJDoc\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_ALF\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AM3(deprecated)\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>component_AMW\n",
       "\n",
       "  (show other bugs)</th>\n",
       "      <th>...</th>\n",
       "      <th>product_WTP EJB Tools</th>\n",
       "      <th>product_WTP Incubator</th>\n",
       "      <th>product_WTP Java EE Tools</th>\n",
       "      <th>product_WTP Releng</th>\n",
       "      <th>product_WTP ServerTools</th>\n",
       "      <th>product_WTP Source Editing</th>\n",
       "      <th>product_WTP Webservices</th>\n",
       "      <th>product_WTP Website</th>\n",
       "      <th>product_Web Tools</th>\n",
       "      <th>product_z_Archived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>I saw this happen on linux, but I'm sure it ha...</td>\n",
       "      <td>2001-10-11</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2001-10-10</td>\n",
       "      <td>[UI] Unzoom occurs if you select file in navig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>DS (10/9/01 4:16:49 PM)Create two classesActio...</td>\n",
       "      <td>2001-10-11</td>\n",
       "      <td>jared_burns</td>\n",
       "      <td>2001-10-10</td>\n",
       "      <td>Breakpoints installed in all classes with \"nam...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  body closed_date  \\\n",
       "392  I saw this happen on linux, but I'm sure it ha...  2001-10-11   \n",
       "251  DS (10/9/01 4:16:49 PM)Create two classesActio...  2001-10-11   \n",
       "\n",
       "    completed_by created_date  \\\n",
       "392      unknown   2001-10-10   \n",
       "251  jared_burns   2001-10-10   \n",
       "\n",
       "                                                 title  \\\n",
       "392  [UI] Unzoom occurs if you select file in navig...   \n",
       "251  Breakpoints installed in all classes with \"nam...   \n",
       "\n",
       "     component_AJBrowser\\n\\n  (show other bugs)\\n  \\\n",
       "392                                             0   \n",
       "251                                             0   \n",
       "\n",
       "     component_AJDoc\\n\\n  (show other bugs)\\n  \\\n",
       "392                                         0   \n",
       "251                                         0   \n",
       "\n",
       "     component_ALF\\n\\n  (show other bugs)\\n  \\\n",
       "392                                       0   \n",
       "251                                       0   \n",
       "\n",
       "     component_AM3(deprecated)\\n\\n  (show other bugs)\\n  \\\n",
       "392                                                  0    \n",
       "251                                                  0    \n",
       "\n",
       "     component_AMW\\n\\n  (show other bugs)\\n           ...            \\\n",
       "392                                       0           ...             \n",
       "251                                       0           ...             \n",
       "\n",
       "     product_WTP EJB Tools\\n\\n  product_WTP Incubator\\n\\n  \\\n",
       "392                          0                          0   \n",
       "251                          0                          0   \n",
       "\n",
       "     product_WTP Java EE Tools\\n\\n  product_WTP Releng\\n\\n  \\\n",
       "392                              0                       0   \n",
       "251                              0                       0   \n",
       "\n",
       "     product_WTP ServerTools\\n\\n  product_WTP Source Editing\\n\\n  \\\n",
       "392                            0                               0   \n",
       "251                            0                               0   \n",
       "\n",
       "     product_WTP Webservices\\n\\n  product_WTP Website\\n\\n  \\\n",
       "392                            0                        0   \n",
       "251                            0                        0   \n",
       "\n",
       "     product_Web Tools\\n\\n  product_z_Archived\\n\\n  \n",
       "392                      0                       0  \n",
       "251                      0                       0  \n",
       "\n",
       "[2 rows x 333 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode the component and team columns\n",
    "df = pd.get_dummies(df, columns=[\"component\", \"product\"], prefix=[\"component\", \"product\"])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77704\n"
     ]
    }
   ],
   "source": [
    "# get list of devs who solve an issue in last three months (from end of train set)\n",
    "active_devs = set()\n",
    "dev_counts = {}\n",
    "filter_date = df.iloc[50000]['closed_date'] - pd.to_timedelta(90, unit='d')\n",
    "for _, row in df.iterrows():\n",
    "    if (row['closed_date'] > filter_date):\n",
    "        dev_counts[row['completed_by']] = dev_counts.get(row['completed_by'], 0) + 1\n",
    "#print(dev_counts)\n",
    "for dev in dev_counts:\n",
    "    if dev_counts[dev] >= 3:\n",
    "        active_devs.add(dev)\n",
    "#print(active_devs)\n",
    "# remove all issues not solved by an active dev\n",
    "df = df[df['completed_by'].isin(active_devs)].reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    # remove punctuation and non-alpha numeric characters\n",
    "    split1 = ' '.join([word for word in re.split('\\W+', text) if word.isalpha()])\n",
    "    # split camel case words apart (necessary for embedded code) and apply stemmer to all words\n",
    "    split2 = ' '.join([stemmer.stem(word) for word in re.sub('(?!^)([A-Z][a-z]+)', r' \\1', split1).split()])\n",
    "    return split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77704, 69040)\n",
      "(77704, 9049)\n"
     ]
    }
   ],
   "source": [
    "# use tf-idf w/ stemming, stop-word removal, and non-alphabetic word removal to generate features\n",
    "df['body'] = df['body'].apply(preprocess)\n",
    "vectorizer_body = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vectorizer_body.fit(df['body'])\n",
    "vector_body = vectorizer_body.transform(df['body'])\n",
    "# summarize encoded vector\n",
    "print(vector_body.shape)\n",
    "df['title'] = df['title'].apply(preprocess)\n",
    "vectorizer_title = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vectorizer_title.fit(df['title'])\n",
    "vector_title = vectorizer_title.transform(df['title'])\n",
    "# summarize encoded vector\n",
    "print(vector_title.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(77704, 78089)\n",
      "(77704, 78089)\n",
      "(77704, 78417)\n"
     ]
    }
   ],
   "source": [
    "print(type(vector_title))\n",
    "print(type(vector_body))\n",
    "sparse_data = hstack((vector_title, vector_body))\n",
    "print(sparse_data.shape)\n",
    "#TODO: these should be numerical features before combining\n",
    "df['completed_by'] = df['completed_by'].astype('category')\n",
    "df['completed_by_encode'] = df['completed_by'].cat.codes\n",
    "#df['completed_by'] = pd.factorize(df['completed_by'])\n",
    "#sparse_data = hstack((sparse_data,np.array(df['completed_by_encode'])[:,None]))\n",
    "print(sparse_data.shape)\n",
    "#TODO: add component and team\n",
    "filter_cols = [col for col in df if col.startswith('product') or col.startswith('component')]\n",
    "sparse_data = hstack((sparse_data,np.array(df[filter_cols]))).tocsr()\n",
    "print(sparse_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_df = df[:size]\\ntest_df = df[size:]\\nX_train = train_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\\ny_train = train_df['completed_by']\\nX_test = test_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\\ny_test = test_df['completed_by']\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 75000\n",
    "X_train = sparse_data[:size]\n",
    "X_test = sparse_data[size:]\n",
    "y_train = df['completed_by'][:size]\n",
    "y_test = df['completed_by'][size:]\n",
    "\n",
    "'''\n",
    "train_df = df[:size]\n",
    "test_df = df[size:]\n",
    "X_train = train_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\n",
    "y_train = train_df['completed_by']\n",
    "X_test = test_df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'completed_by_encode'], axis=1)\n",
    "y_test = test_df['completed_by']\n",
    "'''\n",
    "#print(len(X_train))\n",
    "#print(len(X_test))\n",
    "#X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_top_k(pred_prob, k, actual, labels):\n",
    "    indices = [i for i in range(len(pred_prob))]\n",
    "    top_indices = nlargest(k, indices, key=lambda i: pred_prob[i])\n",
    "    top_choices = set([labels[i] for i in top_indices])\n",
    "    return actual in top_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy: 0.23631656804733728\n",
      "Top 2 Accuracy: 0.35022189349112426\n",
      "Top 3 Accuracy: 0.4145710059171598\n",
      "Top 4 Accuracy: 0.4681952662721893\n",
      "Top 5 Accuracy: 0.5077662721893491\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(25,25))\n",
    "classifier.fit(X_train,y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclassifier = GaussianNB()\\nclassifier.fit(X_train,y_train)\\nclasses = classifier.classes_\\nprint(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\\npred_probs = classifier.predict_proba(X_test)\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclassifier = BernoulliNB()\\nclassifier.fit(X_train,y_train)\\nclasses = classifier.classes_\\nprint(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\\npred_probs = classifier.predict_proba(X_test)\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy: 0.2703402366863905\n",
      "Top 2 Accuracy: 0.3779585798816568\n",
      "Top 3 Accuracy: 0.4145710059171598\n",
      "Top 4 Accuracy: 0.41494082840236685\n",
      "Top 5 Accuracy: 0.41494082840236685\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclassifier = RandomForestClassifier(n_estimators=1000)\\nclassifier.fit(X_train, y_train)\\nclasses = classifier.classes_\\nprint(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\\npred_probs = classifier.predict_proba(X_test)\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "classifier = RandomForestClassifier(n_estimators=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclassifier = SVC(probability=True)\\nclassifier.fit(X_train, y_train)\\nclasses = classifier.classes_\\nprint(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\\npred_probs = classifier.predict_proba(X_test)\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\\ncorrect = 0\\nfor idx, pred in enumerate(pred_probs):\\n    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\\n        correct += 1\\nprint (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: try SVC (might need to do something else)\n",
    "'''\n",
    "classifier = SVC(probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "classes = classifier.classes_\n",
    "print(\"Top 1 Accuracy: \" + str(classifier.score(X_test, y_test)))\n",
    "pred_probs = classifier.predict_proba(X_test)\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 2, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 2 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 3, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 3 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 4, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 4 Accuracy: \" + str((correct/len(y_test))))\n",
    "correct = 0\n",
    "for idx, pred in enumerate(pred_probs):\n",
    "    if is_correct_top_k(pred, 5, y_test[size + idx], classes):\n",
    "        correct += 1\n",
    "print (\"Top 5 Accuracy: \" + str((correct/len(y_test))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snorthov' 'mschorn.eclipse' 'ccc' ... 'Tod_Creasey' 'lynne_kues'\n",
      " 'eclipse']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(25,25))\n",
    "classifier.fit(X_train,y_train)\n",
    "preds = classifier.predict(X_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1fd3881f0a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completed_by'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "actual = test_df['completed_by']\n",
    "pd.value_counts(actual).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['completed_by']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(preds).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_counts = pd.value_counts(df['completed_by'])\n",
    "# list of devs sorted in order of highest contribution\n",
    "sorted_dev_list = overall_counts.index.values\n",
    "pred_counts = pd.value_counts(preds)\n",
    "actual_counts = pd.value_counts(actual)\n",
    "\n",
    "percent_diff = {}\n",
    "vals = []\n",
    "counts = []\n",
    "missing = set()\n",
    "for dev in sorted_dev_list:\n",
    "    if dev in pred_counts:\n",
    "        val = 100 * (pred_counts[dev] - actual_counts[dev]) / (actual_counts[dev])\n",
    "        percent_diff[dev] = val\n",
    "        counts.append(overall_counts[dev])\n",
    "        vals.append(val)\n",
    "    else:\n",
    "        missing.add(dev)\n",
    "print(\"devs not included in predictions: \")\n",
    "print(list(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(percent_diff.keys(), percent_diff.values())\n",
    "plt.title(\"Percent Difference Prediction Rate vs. Actual Rate\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "del vals[2]\n",
    "del counts[2]\n",
    "# create best fit line\n",
    "z = np.polyfit(x=counts, y=vals, deg=1)\n",
    "p = np.poly1d(z)\n",
    "trend_line = p(counts)\n",
    "# test best fit\n",
    "yhat = trend_line    \n",
    "ybar = np.sum(vals)/len(vals)\n",
    "ssreg = np.sum((yhat-ybar)**2) \n",
    "sstot = np.sum((vals - ybar)**2)\n",
    "print(\"R^2: \" + str(ssreg / sstot))\n",
    "# create plots\n",
    "plt.scatter(counts, vals)\n",
    "plt.title(\"Percent Difference Between Prediction Rate and Actual Rate vs. Issue Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Number of Issues Solved\")\n",
    "plt.ylabel(\"% diff. pred rate and actual rate\")\n",
    "plt.plot(counts, trend_line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: look at assigning open issues and seeing what overspecialization problem would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
